Nous avons vu deux manière de résoudre l'équation de poisson. Nous allons maintenant présenter une troisième méthode  : \\
\begin{center}
Le méthode de Douglas
\end{center}
Commençons par remarquer que l'équation que nous souhaitons résoudre est en réalité un problème d'optimisation avec contraintes.\\
Transformons ce problème, en un problème sans contraintes. Soit $I$, l'image à retrouver
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{D'un problème avec contraintes...}
Remarquons que le problème initial est un problème d'optimisation avec contraintes. En effet, nous voulons minimiser $\int_\Omega ||\nabla I-\nabla S||^2$. Avec la contrainte suivante : $I = T$ en dehors du domaine. \\
Ce problème d'optimisation peut donc être résolu à l'aide de différents algorithmes, mais avant ça, transformons le en un problème sans contraintes. 

\subsubsection{... À un problème sans contraintes}
En utilisant des fonctions de pénalisation nous remarquons aisément que ce problème peut être ramené à un problème sans contraintes. Réécrivons donc le problème : 
\begin{equation*}
\begin{aligned}{}
    min \int_\Omega ||\nabla I - \nabla S||^2 + \mathbb{1} _{D \backslash \Omega } (I) \\
    \end{aligned}
\end{equation*}{}
avec 
\begin{equation*}
\mathbb{1}_{ D \backslash \Omega }(I) =
	\left\{
	\begin{aligned}{}
	0 \ si\  I \in T \backslash \Omega \\
	+ \infty \ sinon
    \end{aligned}
    \right.
\end{equation*}{}
Nous avons bien équivalence entre notre problème sans contraintes et le problèmes (1). En effet, si I $\in T \backslash \Omega $, alors l'indicatrice vaut 0 et nous devons juste résoudre $min \int_\Omega ||\nabla I - \nabla S||^2 $. Si au contraire $I \notin T\backslash \Omega$, alors nous devons minimiser quelque chose qui vaut plus $+\infty$. Le minimum n'existe pas. En effet, la condition I = T en dehors du domaine n'étant pas respectée, le problème n'a pas de solution. Ce problème sans contraintes, traduit bien celui avec contraintes. Nous pouvons donc essayer de résoudre celui-ci, numériquement. Dans la suite nous prouverons que cette fonction est bien convexe,et par conséquent, qu'elle admet bien un minimum. 
\subsubsection{La convexité...}
Montrons que K est convexe. 
Soit u et v deux images appartenant à K, alors, les pixels de u et de v, se situant à l'extérieur de $\Omega$, coïncident avec les pixels de T. \\
Considérons maintenant une nouvelle image : 
\begin{center}
$M = \lambda u+(1-\lambda)v$
\end{center}
Les pixels de u et v coïncidant avec ceux de T, nous pouvons écrire les pixels de $M$, n'appartenant pas à $\Omega$ de la manière suivante. \\
Pour $i,j \notin \Omega$ : \\
\begin{center}
$M_{i,j} = \lambda t_{i,j}+(1-\lambda) t_{i,j} = t_{i,j}$
\end{center}
Ainsi, les pixels de M n'appartenant pas à $\Omega$, coïncident avec T. \\
M appartient bien à K. Et K est donc convexe.\\
K étant convexe, et non vide, (T en particulier appartient à K), alors la fonction $\mathbb{1}_K(I)$ est convexe. \\
Enfin montrons la convexité de $||\nabla I-\nabla S||^2$.
La norme étant une fonction convexe et croissante, alors la fonction : $||.||^2$ est elle aussi convexe. 
Nous avons donc $f et g$, convexes, ainsi, la fonction $F =f+g$ est elle aussi convexe. Elle admet donc un minimum. Nous pouvons ainsi résoudre numériquement ce problème.
\subsubsection{... Pour utiliser l'algorithme de Douglas...}
L'algorithme que nous allons utiliser est l'algorithme de Douglas-Rachford. Cet algorithme permet d'approcher le minimum  d'une fonction $F = f+g$, f et g étant des fonctions convexes, comme montré dans la partie précédente, nous pouvons utiliser cet algorithme.
\paragraph{L'algorithme}
À chaque itération on a : 
\begin{center}
$x_{k+1} = prox_f(y_k)$\\
$y_{k+1} = y_k+prox_g(2x_x{k+1}-y_k)-x_{k+1}$
\end{center}{}
Nous devons donc calculer les opérateurs proximaux. 
\subsubsection{... Avec les opérateurs proximaux ...}
Un opérateur proximal est défini comme suit : 
\begin{center}
\begin{equation*}
\begin{aligned}
prox_f(x) = argmin_u \left\{ \frac{||u-x||^2}{2}+ f(u)\right\}
\end{aligned}
\end{equation*}
\end{center}
\paragraph{Opérateur proximal de f}
\begin{equation*}
prox_f(x) = argmin_u\left\{\frac{||u-x||^2}{2}+||\nabla u -\nabla S ||^2 \right\}
\end{equation*}
Afin de faciliter les notations notons :
\begin{equation*}
h(u) = \frac{||u-x||^2}{2}+||\nabla u -\nabla S ||^2
\end{equation*} 
Nous cherchons donc 
\begin{center}
$argmin_u h(u)$
\end{center}
ie. u qui minimise la fonction h, autrement dit, un u qui annule le gradient de h.\\
En utilisant Taylor Young, 

\begin{equation*}
\begin{aligned}
h(u+k) -h(u) &= \frac{||u+k-x||^2}{2}+||\nabla  (u+k) -\nabla S ||^2- \frac{||u-x||^2}{2}-||\nabla u -\nabla S ||^2\\
& = \frac{||k||^2+2\left<u-x,k\right>}{2}+||\nabla k||^2+2\left<\nabla u-\nabla S, \nabla k\right>\\
& = O(||k||^2)+\left<u-x,k\right>-2\left<div(\nabla u-\nabla S), k\right>\\
& = \left<u-x-2div(\nabla u-\nabla S), k\right>\\
\end{aligned}
\end{equation*}
Par identification 
\begin{equation*}
\begin{aligned}
\nabla h(u) &= u-x-2div(\nabla u - \nabla S)\\
& = u-x-2(\Delta u -\Delta S)
\end{aligned}
\end{equation*}
En résolvant $\nabla h(u) = 0$, nous pourrons trouver  : $prox_f(x) $. \\	
	\begin{equation*}
		\begin{aligned}
		\nabla h(u) &= 0\\
		u-x-2(\Delta u -\Delta S) & =0\\
		u-2\Delta u = x-2\Delta S
		\end{aligned}
\end{equation*}
Afin de trouver u, nous utiliserons la méthode des différences finies. En discrétisant le laplacien de u, comme nous l'avons vu dans la section (1) : 
\begin{equation*}
 -2u(x+1,y) -2 u(x-1,y)-2u(x,y+1) -2 u(x,y-1) +9\times u(x,y) =  y_k-  2\Delta S(x,y)
\end{equation*}
En mettant ce système sous forme matricielle nous pourrons approcher u en faisant une inversion matricielle. 
Nous pouvons donc numériquement approcher $prox_f(x)$.

\paragraph{Opérateur proximal de g}
g étant la fonction indicatrice suivante : 
\begin{equation*}
\mathbb{1}_{ D \backslash \Omega }(I) =
	\left\{
	\begin{aligned}{}
	0 \ si\  I \in T \backslash \Omega \\
	+ \infty \ sinon
    \end{aligned}
    \right.
\end{equation*}{}
Nous avons donc 
\begin{equation*}
prox_g(x) =  argmin_u\left\{\frac{||u-x||^2}{2}+ \mathbb{1}_K(u)\right\}
\end{equation*}
Nous savons que $prox_g(x)$ existe puisque la fonction g est convexe et la fonction norme est elle aussi convexe.
Notons $h(u) = \frac{||u-x||^2}{2}+ \mathbb{1}_K(u)$ 
Comme nous l'avons cette fonction n'admet un minimum que si $u \in K$. Supposons donc $u \in K$. Alors chercher $argmin_u \left\{h(u)\right\}$ est équivalent à chercher $argmin_{u\in K} \left\{\frac{||u-x||^2}{2}\right\}$.\\
Dans notre cas, u $prox_g(x) = L$.
L étant une image appartenant à K, et dont les pixels à l'intérieur de $\Omega$ coïncident avec x.\\
\subsubsection{Convergence de l'algorithme vers la solution}

\subsubsection{Résultats obtenus}

\subsubsection{Temps et coût de l'algorithme}




